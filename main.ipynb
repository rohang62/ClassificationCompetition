{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/rohang62/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import dataextraction as de\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import re,string,unicodedata\n",
    "from string import punctuation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = de.extract_data('data/train.jsonl')\n",
    "train = de.parse_json(data, True)\n",
    "\n",
    "data = de.extract_data('data/test.jsonl')\n",
    "test = de.parse_json(data, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "1800\n"
     ]
    }
   ],
   "source": [
    "print(len(train.index))\n",
    "print(len(test.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "punctuation = list(string.punctuation)\n",
    "stop.update(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub(r'http\\S+', '', text)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    final_text = []\n",
    "    for i in text.split():\n",
    "        if i.strip().lower() not in stop and i != \"@USER\":\n",
    "            final_text.append(i.strip().lower())\n",
    "    return \" \".join(final_text)\n",
    "\n",
    "def denoise_text(text):\n",
    "    text = strip_html(text)\n",
    "    text = remove_between_square_brackets(text)\n",
    "    text = remove_stopwords(text)\n",
    "    return text\n",
    "\n",
    "train['response']=train['response'].apply(denoise_text)\n",
    "test['response']=test['response'].apply(denoise_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SARCASM</td>\n",
       "      <td>get .. obviously care would've moved right alo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SARCASM</td>\n",
       "      <td>trying protest talking labels label wtf make em</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SARCASM</td>\n",
       "      <td>makes insane money movies einstein #learnhowth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SARCASM</td>\n",
       "      <td>meanwhile trump even release sat scores wharto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SARCASM</td>\n",
       "      <td>pretty sure anti-lincoln crowd claimed democra...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                           response\n",
       "0  SARCASM  get .. obviously care would've moved right alo...\n",
       "0  SARCASM    trying protest talking labels label wtf make em\n",
       "0  SARCASM  makes insane money movies einstein #learnhowth...\n",
       "0  SARCASM  meanwhile trump even release sat scores wharto...\n",
       "0  SARCASM  pretty sure anti-lincoln crowd claimed democra..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_ft.txt', 'w') as f:\n",
    "    for each_text, each_label in zip(train['response'], train['label']):\n",
    "        f.writelines(f'__label__{each_label} {each_text}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "model = fasttext.train_supervised('train_ft.txt', epoch=300, lr=1, wordNgrams=5, verbose=2, minCount=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "def print_results(sample_size, precision, recall):\n",
    "    precision   = round(precision, 2)\n",
    "    recall      = round(recall, 2)\n",
    "    print(f'{precision}')\n",
    "    print(f'{recall}')\n",
    "\n",
    "print_results(*model.test('train_ft.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_is_sarcastic(text):\n",
    "    return model.predict(text, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm answer.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('answer.txt', 'w') as f:\n",
    "    for each_id, each_text in zip(test['id'], test['response']):\n",
    "        labels, probs = predict_is_sarcastic(f'{each_text}')\n",
    "        l = None\n",
    "        if (probs[0] > probs[1] and labels[0] == \"__label__SARCASM\"):\n",
    "            l = \"SARCASM\"\n",
    "        elif (probs[0] > probs[1] and labels[0] != \"__label__SARCASM\"): \n",
    "            l = \"NOT_SARCASM\"\n",
    "        elif (probs[0] < probs[1] and labels[0] == \"__label__SARCASM\"): \n",
    "            l = \"NOT_SARCASM\"\n",
    "        else:\n",
    "            l = \"SARCASM\"\n",
    "        f.writelines(f'{each_id},{l}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import torch\n",
    "import transformers as ppb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.replace(\"SARCASM\", 1)\n",
    "train = train.replace(\"NOT_SARCASM\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = train['response'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 0\n",
    "for i in tokenized.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 73)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = np.where(padded != 0, 1, 0)\n",
    "attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor(padded)  \n",
    "attention_mask = torch.tensor(attention_mask)\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states = model(input_ids, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = last_hidden_states[0][:,0,:].numpy()\n",
    "train_labels = train['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = test['response'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 0\n",
    "for i in tokenized.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1800, 49)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = np.where(padded != 0, 1, 0)\n",
    "attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor(padded)  \n",
    "attention_mask = torch.tensor(attention_mask)\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states = model(input_ids, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=10000)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_clf = LogisticRegression(max_iter = 10000)\n",
    "lr_clf.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8128"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_clf.score(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = last_hidden_states[0][:,0,:].numpy()\n",
    "test_pred = lr_clf.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_features, 30)\n",
    "        self.fc2 = nn.Linear(30, n_features)\n",
    "        self.fc3 = nn.Linear(n_features, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return torch.sigmoid(self.fc3(x))\n",
    "\n",
    "\n",
    "net = Net(train_features.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(train_features).float()\n",
    "y_train = torch.squeeze(torch.from_numpy(train_labels.to_numpy()).float())\n",
    "X_test = torch.from_numpy(test_features).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "X_train = X_train.to(device)\n",
    "y_train = y_train.to(device)\n",
    "X_test = X_test.to(device)\n",
    "net = net.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(y_true, y_pred):\n",
    "    predicted = y_pred.ge(.5).view(-1)\n",
    "    return (y_true == predicted).sum().float() / len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 Train set - loss: 0.692, accuracy: 0.581\n",
      "epoch 10 Train set - loss: 0.564, accuracy: 0.724\n",
      "epoch 20 Train set - loss: 0.52, accuracy: 0.739\n",
      "epoch 30 Train set - loss: 0.502, accuracy: 0.751\n",
      "epoch 40 Train set - loss: 0.488, accuracy: 0.76\n",
      "epoch 50 Train set - loss: 0.476, accuracy: 0.768\n",
      "epoch 60 Train set - loss: 0.463, accuracy: 0.778\n",
      "epoch 70 Train set - loss: 0.45, accuracy: 0.788\n",
      "epoch 80 Train set - loss: 0.438, accuracy: 0.796\n",
      "epoch 90 Train set - loss: 0.427, accuracy: 0.798\n",
      "epoch 100 Train set - loss: 0.414, accuracy: 0.809\n",
      "epoch 110 Train set - loss: 0.406, accuracy: 0.814\n",
      "epoch 120 Train set - loss: 0.393, accuracy: 0.825\n",
      "epoch 130 Train set - loss: 0.383, accuracy: 0.833\n",
      "epoch 140 Train set - loss: 0.375, accuracy: 0.83\n",
      "epoch 150 Train set - loss: 0.362, accuracy: 0.845\n",
      "epoch 160 Train set - loss: 0.351, accuracy: 0.849\n",
      "epoch 170 Train set - loss: 0.344, accuracy: 0.847\n",
      "epoch 180 Train set - loss: 0.335, accuracy: 0.857\n",
      "epoch 190 Train set - loss: 0.322, accuracy: 0.862\n",
      "epoch 200 Train set - loss: 0.31, accuracy: 0.871\n",
      "epoch 210 Train set - loss: 0.301, accuracy: 0.877\n",
      "epoch 220 Train set - loss: 0.295, accuracy: 0.877\n",
      "epoch 230 Train set - loss: 0.288, accuracy: 0.882\n",
      "epoch 240 Train set - loss: 0.277, accuracy: 0.885\n",
      "epoch 250 Train set - loss: 0.276, accuracy: 0.885\n",
      "epoch 260 Train set - loss: 0.253, accuracy: 0.903\n",
      "epoch 270 Train set - loss: 0.25, accuracy: 0.9\n",
      "epoch 280 Train set - loss: 0.242, accuracy: 0.908\n",
      "epoch 290 Train set - loss: 0.235, accuracy: 0.907\n",
      "epoch 300 Train set - loss: 0.227, accuracy: 0.916\n",
      "epoch 310 Train set - loss: 0.217, accuracy: 0.919\n",
      "epoch 320 Train set - loss: 0.207, accuracy: 0.927\n",
      "epoch 330 Train set - loss: 0.198, accuracy: 0.931\n",
      "epoch 340 Train set - loss: 0.201, accuracy: 0.925\n",
      "epoch 350 Train set - loss: 0.191, accuracy: 0.93\n",
      "epoch 360 Train set - loss: 0.18, accuracy: 0.941\n",
      "epoch 370 Train set - loss: 0.178, accuracy: 0.939\n",
      "epoch 380 Train set - loss: 0.168, accuracy: 0.946\n",
      "epoch 390 Train set - loss: 0.159, accuracy: 0.95\n",
      "epoch 400 Train set - loss: 0.161, accuracy: 0.945\n",
      "epoch 410 Train set - loss: 0.148, accuracy: 0.955\n",
      "epoch 420 Train set - loss: 0.142, accuracy: 0.958\n",
      "epoch 430 Train set - loss: 0.147, accuracy: 0.95\n",
      "epoch 440 Train set - loss: 0.151, accuracy: 0.947\n",
      "epoch 450 Train set - loss: 0.127, accuracy: 0.965\n",
      "epoch 460 Train set - loss: 0.125, accuracy: 0.962\n",
      "epoch 470 Train set - loss: 0.119, accuracy: 0.965\n",
      "epoch 480 Train set - loss: 0.113, accuracy: 0.97\n",
      "epoch 490 Train set - loss: 0.11, accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "def round_tensor(t, decimal_places=3):\n",
    "    return round(t.item(), decimal_places)\n",
    "for epoch in range(500):\n",
    "    y_pred = net(X_train)\n",
    "    y_pred = torch.squeeze(y_pred)\n",
    "    train_loss = criterion(y_pred, y_train)\n",
    "    if epoch % 10 == 0:\n",
    "        train_acc = calculate_accuracy(y_train, y_pred)\n",
    "        print(f'''epoch {epoch} Train set - loss: {round_tensor(train_loss)}, accuracy: {round_tensor(train_acc)}''')\n",
    "    optimizer.zero_grad()\n",
    "    train_loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0727],\n",
      "        [0.7206],\n",
      "        [0.7569],\n",
      "        ...,\n",
      "        [0.7691],\n",
      "        [0.0211],\n",
      "        [0.0344]], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "y_test = net(X_test)\n",
    "print(y_test)\n",
    "y_labels = []\n",
    "for i in range(y_test.shape[0]):\n",
    "    if y_test[i] < 0.5:\n",
    "        y_labels.append(\"NOT_SARCASM\")\n",
    "    else:\n",
    "        y_labels.append(\"SARCASM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('answer.txt', 'w') as f:\n",
    "    for tid, pred in zip(test['id'], y_labels):\n",
    "        f.writelines(f'{tid},{pred}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import savetxt\n",
    "savetxt('train_features.csv', train_features, delimiter=',')\n",
    "savetxt('train_labels.csv', train_labels.to_numpy(), delimiter=',')\n",
    "savetxt('test_features.csv', train_labels.to_numpy(), delimiter=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
