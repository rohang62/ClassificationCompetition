{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/rohang62/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import dataextraction as de\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import re,string,unicodedata\n",
    "from string import punctuation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = de.extract_data('data/train.jsonl')\n",
    "train = de.parse_json(data, True)\n",
    "\n",
    "data = de.extract_data('data/test.jsonl')\n",
    "test = de.parse_json(data, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "1800\n"
     ]
    }
   ],
   "source": [
    "print(len(train.index))\n",
    "print(len(test.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "punctuation = list(string.punctuation)\n",
    "stop.update(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub(r'http\\S+', '', text)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    final_text = []\n",
    "    for i in text.split():\n",
    "        if i.strip().lower() not in stop and i != \"@USER\":\n",
    "            final_text.append(i.strip().lower())\n",
    "    return \" \".join(final_text)\n",
    "\n",
    "def denoise_text(text):\n",
    "    text = strip_html(text)\n",
    "    text = remove_between_square_brackets(text)\n",
    "    text = remove_stopwords(text)\n",
    "    return text\n",
    "\n",
    "train['response']=train['response'].apply(denoise_text)\n",
    "test['response']=test['response'].apply(denoise_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SARCASM</td>\n",
       "      <td>get .. obviously care would've moved right alo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SARCASM</td>\n",
       "      <td>trying protest talking labels label wtf make em</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SARCASM</td>\n",
       "      <td>makes insane money movies einstein #learnhowth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SARCASM</td>\n",
       "      <td>meanwhile trump even release sat scores wharto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SARCASM</td>\n",
       "      <td>pretty sure anti-lincoln crowd claimed democra...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                           response\n",
       "0  SARCASM  get .. obviously care would've moved right alo...\n",
       "0  SARCASM    trying protest talking labels label wtf make em\n",
       "0  SARCASM  makes insane money movies einstein #learnhowth...\n",
       "0  SARCASM  meanwhile trump even release sat scores wharto...\n",
       "0  SARCASM  pretty sure anti-lincoln crowd claimed democra..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_ft.txt', 'w') as f:\n",
    "    for each_text, each_label in zip(train['response'], train['label']):\n",
    "        f.writelines(f'__label__{each_label} {each_text}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "model = fasttext.train_supervised('train_ft.txt', epoch=300, lr=1, wordNgrams=5, verbose=2, minCount=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "def print_results(sample_size, precision, recall):\n",
    "    precision   = round(precision, 2)\n",
    "    recall      = round(recall, 2)\n",
    "    print(f'{precision}')\n",
    "    print(f'{recall}')\n",
    "\n",
    "print_results(*model.test('train_ft.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_is_sarcastic(text):\n",
    "    return model.predict(text, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm answer.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('answer.txt', 'w') as f:\n",
    "    for each_id, each_text in zip(test['id'], test['response']):\n",
    "        labels, probs = predict_is_sarcastic(f'{each_text}')\n",
    "        l = None\n",
    "        if (probs[0] > probs[1] and labels[0] == \"__label__SARCASM\"):\n",
    "            l = \"SARCASM\"\n",
    "        elif (probs[0] > probs[1] and labels[0] != \"__label__SARCASM\"): \n",
    "            l = \"NOT_SARCASM\"\n",
    "        elif (probs[0] < probs[1] and labels[0] == \"__label__SARCASM\"): \n",
    "            l = \"NOT_SARCASM\"\n",
    "        else:\n",
    "            l = \"SARCASM\"\n",
    "        f.writelines(f'{each_id},{l}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import torch\n",
    "import transformers as ppb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.replace(\"SARCASM\", 1)\n",
    "train = train.replace(\"NOT_SARCASM\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = train['response'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 0\n",
    "for i in tokenized.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 73)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = np.where(padded != 0, 1, 0)\n",
    "attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor(padded)  \n",
    "attention_mask = torch.tensor(attention_mask)\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states = model(input_ids, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = last_hidden_states[0][:,0,:].numpy()\n",
    "train_labels = train['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = test['response'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 0\n",
    "for i in tokenized.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1800, 49)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = np.where(padded != 0, 1, 0)\n",
    "attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor(padded)  \n",
    "attention_mask = torch.tensor(attention_mask)\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states = model(input_ids, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=10000)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_clf = LogisticRegression(max_iter = 10000)\n",
    "lr_clf.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8128"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_clf.score(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = last_hidden_states[0][:,0,:].numpy()\n",
    "test_pred = lr_clf.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_features, 30)\n",
    "        self.fc2 = nn.Linear(30, n_features)\n",
    "        self.fc3 = nn.Linear(n_features, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return torch.sigmoid(self.fc3(x))\n",
    "\n",
    "\n",
    "net = Net(train_features.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(train_features).float()\n",
    "y_train = torch.squeeze(torch.from_numpy(train_labels.to_numpy()).float())\n",
    "X_test = torch.from_numpy(test_features).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "5000\n",
      "torch.Size([5000, 768])\n"
     ]
    }
   ],
   "source": [
    "print(y_train)\n",
    "print(len(y_train))\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "X_train = X_train.to(device)\n",
    "y_train = y_train.to(device)\n",
    "X_test = X_test.to(device)\n",
    "net = net.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(y_true, y_pred):\n",
    "    predicted = y_pred.ge(.5).view(-1)\n",
    "    return (y_true == predicted).sum().float() / len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 Train set - loss: 0.692, accuracy: 0.584\n",
      "epoch 10 Train set - loss: 0.579, accuracy: 0.725\n",
      "epoch 20 Train set - loss: 0.523, accuracy: 0.737\n",
      "epoch 30 Train set - loss: 0.505, accuracy: 0.748\n",
      "epoch 40 Train set - loss: 0.492, accuracy: 0.756\n",
      "epoch 50 Train set - loss: 0.479, accuracy: 0.768\n",
      "epoch 60 Train set - loss: 0.466, accuracy: 0.778\n",
      "epoch 70 Train set - loss: 0.453, accuracy: 0.787\n",
      "epoch 80 Train set - loss: 0.439, accuracy: 0.798\n",
      "epoch 90 Train set - loss: 0.425, accuracy: 0.801\n",
      "epoch 100 Train set - loss: 0.412, accuracy: 0.814\n",
      "epoch 110 Train set - loss: 0.403, accuracy: 0.815\n",
      "epoch 120 Train set - loss: 0.388, accuracy: 0.823\n",
      "epoch 130 Train set - loss: 0.374, accuracy: 0.836\n",
      "epoch 140 Train set - loss: 0.365, accuracy: 0.838\n",
      "epoch 150 Train set - loss: 0.349, accuracy: 0.846\n",
      "epoch 160 Train set - loss: 0.337, accuracy: 0.853\n",
      "epoch 170 Train set - loss: 0.328, accuracy: 0.856\n",
      "epoch 180 Train set - loss: 0.319, accuracy: 0.864\n",
      "epoch 190 Train set - loss: 0.309, accuracy: 0.864\n",
      "epoch 200 Train set - loss: 0.293, accuracy: 0.876\n",
      "epoch 210 Train set - loss: 0.28, accuracy: 0.886\n",
      "epoch 220 Train set - loss: 0.27, accuracy: 0.891\n",
      "epoch 230 Train set - loss: 0.26, accuracy: 0.897\n",
      "epoch 240 Train set - loss: 0.25, accuracy: 0.901\n",
      "epoch 250 Train set - loss: 0.241, accuracy: 0.905\n",
      "epoch 260 Train set - loss: 0.233, accuracy: 0.912\n",
      "epoch 270 Train set - loss: 0.224, accuracy: 0.915\n",
      "epoch 280 Train set - loss: 0.219, accuracy: 0.918\n",
      "epoch 290 Train set - loss: 0.229, accuracy: 0.904\n",
      "epoch 300 Train set - loss: 0.203, accuracy: 0.921\n",
      "epoch 310 Train set - loss: 0.196, accuracy: 0.929\n",
      "epoch 320 Train set - loss: 0.189, accuracy: 0.932\n",
      "epoch 330 Train set - loss: 0.198, accuracy: 0.925\n",
      "epoch 340 Train set - loss: 0.176, accuracy: 0.936\n",
      "epoch 350 Train set - loss: 0.173, accuracy: 0.936\n",
      "epoch 360 Train set - loss: 0.166, accuracy: 0.941\n",
      "epoch 370 Train set - loss: 0.162, accuracy: 0.943\n",
      "epoch 380 Train set - loss: 0.156, accuracy: 0.945\n",
      "epoch 390 Train set - loss: 0.152, accuracy: 0.946\n",
      "epoch 400 Train set - loss: 0.144, accuracy: 0.95\n",
      "epoch 410 Train set - loss: 0.215, accuracy: 0.903\n",
      "epoch 420 Train set - loss: 0.148, accuracy: 0.944\n",
      "epoch 430 Train set - loss: 0.137, accuracy: 0.954\n",
      "epoch 440 Train set - loss: 0.13, accuracy: 0.957\n",
      "epoch 450 Train set - loss: 0.124, accuracy: 0.962\n",
      "epoch 460 Train set - loss: 0.12, accuracy: 0.964\n",
      "epoch 470 Train set - loss: 0.116, accuracy: 0.965\n",
      "epoch 480 Train set - loss: 0.112, accuracy: 0.967\n",
      "epoch 490 Train set - loss: 0.11, accuracy: 0.967\n",
      "epoch 500 Train set - loss: 0.109, accuracy: 0.969\n",
      "epoch 510 Train set - loss: 0.102, accuracy: 0.971\n",
      "epoch 520 Train set - loss: 0.099, accuracy: 0.972\n",
      "epoch 530 Train set - loss: 0.101, accuracy: 0.968\n",
      "epoch 540 Train set - loss: 0.091, accuracy: 0.978\n",
      "epoch 550 Train set - loss: 0.088, accuracy: 0.978\n",
      "epoch 560 Train set - loss: 0.085, accuracy: 0.979\n",
      "epoch 570 Train set - loss: 0.095, accuracy: 0.97\n",
      "epoch 580 Train set - loss: 0.087, accuracy: 0.978\n",
      "epoch 590 Train set - loss: 0.076, accuracy: 0.983\n",
      "epoch 600 Train set - loss: 0.073, accuracy: 0.984\n",
      "epoch 610 Train set - loss: 0.072, accuracy: 0.985\n",
      "epoch 620 Train set - loss: 0.067, accuracy: 0.986\n",
      "epoch 630 Train set - loss: 0.116, accuracy: 0.951\n",
      "epoch 640 Train set - loss: 0.523, accuracy: 0.818\n",
      "epoch 650 Train set - loss: 0.254, accuracy: 0.887\n",
      "epoch 660 Train set - loss: 0.131, accuracy: 0.949\n",
      "epoch 670 Train set - loss: 0.103, accuracy: 0.965\n",
      "epoch 680 Train set - loss: 0.086, accuracy: 0.978\n",
      "epoch 690 Train set - loss: 0.077, accuracy: 0.982\n",
      "epoch 700 Train set - loss: 0.071, accuracy: 0.986\n",
      "epoch 710 Train set - loss: 0.067, accuracy: 0.988\n",
      "epoch 720 Train set - loss: 0.065, accuracy: 0.988\n",
      "epoch 730 Train set - loss: 0.063, accuracy: 0.988\n",
      "epoch 740 Train set - loss: 0.061, accuracy: 0.989\n",
      "epoch 750 Train set - loss: 0.059, accuracy: 0.989\n",
      "epoch 760 Train set - loss: 0.058, accuracy: 0.99\n",
      "epoch 770 Train set - loss: 0.056, accuracy: 0.99\n",
      "epoch 780 Train set - loss: 0.055, accuracy: 0.991\n",
      "epoch 790 Train set - loss: 0.053, accuracy: 0.992\n",
      "epoch 800 Train set - loss: 0.051, accuracy: 0.992\n",
      "epoch 810 Train set - loss: 0.049, accuracy: 0.992\n",
      "epoch 820 Train set - loss: 0.048, accuracy: 0.993\n",
      "epoch 830 Train set - loss: 0.046, accuracy: 0.994\n",
      "epoch 840 Train set - loss: 0.044, accuracy: 0.994\n",
      "epoch 850 Train set - loss: 0.042, accuracy: 0.994\n",
      "epoch 860 Train set - loss: 0.041, accuracy: 0.995\n",
      "epoch 870 Train set - loss: 0.039, accuracy: 0.995\n",
      "epoch 880 Train set - loss: 0.038, accuracy: 0.996\n",
      "epoch 890 Train set - loss: 0.036, accuracy: 0.996\n",
      "epoch 900 Train set - loss: 0.034, accuracy: 0.996\n",
      "epoch 910 Train set - loss: 0.033, accuracy: 0.996\n",
      "epoch 920 Train set - loss: 0.031, accuracy: 0.996\n",
      "epoch 930 Train set - loss: 0.03, accuracy: 0.996\n",
      "epoch 940 Train set - loss: 0.029, accuracy: 0.997\n",
      "epoch 950 Train set - loss: 0.027, accuracy: 0.997\n",
      "epoch 960 Train set - loss: 0.026, accuracy: 0.998\n",
      "epoch 970 Train set - loss: 0.025, accuracy: 0.998\n",
      "epoch 980 Train set - loss: 0.024, accuracy: 0.999\n",
      "epoch 990 Train set - loss: 0.023, accuracy: 0.999\n"
     ]
    }
   ],
   "source": [
    "def round_tensor(t, decimal_places=3):\n",
    "    return round(t.item(), decimal_places)\n",
    "for epoch in range(300):\n",
    "    y_pred = net(X_train)\n",
    "    y_pred = torch.squeeze(y_pred)\n",
    "    train_loss = criterion(y_pred, y_train)\n",
    "    if epoch % 10 == 0:\n",
    "        train_acc = calculate_accuracy(y_train, y_pred)\n",
    "        print(f'''epoch {epoch} Train set - loss: {round_tensor(train_loss)}, accuracy: {round_tensor(train_acc)}''')\n",
    "    optimizer.zero_grad()\n",
    "    train_loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[8.4971e-07],\n",
      "        [8.5764e-02],\n",
      "        [7.7847e-01],\n",
      "        ...,\n",
      "        [8.4523e-03],\n",
      "        [1.6363e-05],\n",
      "        [2.7454e-06]], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "y_test = net(X_test)\n",
    "print(y_test)\n",
    "y_labels = []\n",
    "for i in range(y_test.shape[0]):\n",
    "    if y_test[i] < 0.5:\n",
    "        y_labels.append(\"NOT_SARCASM\")\n",
    "    else:\n",
    "        y_labels.append(\"SARCASM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('answer.txt', 'w') as f:\n",
    "    for tid, pred in zip(test['id'], y_labels):\n",
    "        f.writelines(f'{tid},{pred}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import savetxt\n",
    "savetxt('train_features.csv', train_features, delimiter=',')\n",
    "savetxt('train_labels.csv', train_labels.to_numpy(), delimiter=',')\n",
    "savetxt('test_features.csv', train_labels.to_numpy(), delimiter=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
